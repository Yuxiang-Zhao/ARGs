import pandas as pd
import random
from ete3 import Tree
import numpy as np
from collections import Counter

# 解析系统发育树文件，将其加载为Tree对象
def parse_tree(tree_file):
    tree = Tree(tree_file, format=1)
    return tree

# 根据系统发育距离对序列进行分组
def group_sequences_by_distance(tree, sequences):
    groups = {}
    for seq in sequences:
        node = tree.search_nodes(name=seq)
        if node:
            distance = node[0].dist
            if distance not in groups:
                groups[distance] = []
            groups[distance].append(seq)
    return groups

# 从不同的系统发育组中挑选序列，优先挑选不同组中的序列
def sample_from_groups_by_habitat(groups, sample_size_per_habitat, habitats, tree):
    sampled_sequences = []

    # 获取不同生境的序列
    habitat_sequences = {habitat: [] for habitat in set(habitats.values())}
    for group, sequences in groups.items():
        for seq in sequences:
            habitat_sequences[habitats[seq]].append(seq)

    # 遍历每个生境，按每个生境单独抽样，优先不同组
    for habitat, sequences in habitat_sequences.items():
        group_sequences = {group: [seq for seq in sequences if seq in groups[group]] for group in groups}
        selected = []

        # 先从不同组里抽取，直到达到要求的数量
        for group, seq_list in sorted(group_sequences.items(), key=lambda x: x[0]):
            if len(selected) >= sample_size_per_habitat:
                break
            available_size = sample_size_per_habitat - len(selected)
            if len(seq_list) <= available_size:
                selected.extend(seq_list)
            else:
                selected.extend(random.sample(seq_list, available_size))

        # 如果不足，从相同组里继续补充，轮流抽取
        if len(selected) < sample_size_per_habitat:
            remaining_size = sample_size_per_habitat - len(selected)
            group_order = sorted(group_sequences.items(), key=lambda x: x[0])

            # 轮流从不同组中抽取，直到填满
            while remaining_size > 0:
                for group, seq_list in group_order:
                    remaining_sequences = [seq for seq in seq_list if seq not in selected]
                    if remaining_sequences:
                        selected.append(random.choice(remaining_sequences))
                        remaining_size -= 1
                    if remaining_size == 0:
                        break

        sampled_sequences.extend(selected)

    # 按照系统发育树的顺序对序列进行排序
    sampled_sequences = sort_sequences_by_tree(tree, sampled_sequences)

    return sampled_sequences

# 根据系统发育树中的顺序对抽取的序列进行排序
def sort_sequences_by_tree(tree, sequences):
    # 获取系统发育树上所有节点的顺序
    tree_order = tree.get_leaf_names()
    
    # 对抽取到的序列按照它们在树中的位置进行排序
    sorted_sequences = sorted(sequences, key=lambda seq: tree_order.index(seq))
    
    return sorted_sequences

# 对系统发育距离为0的序列组进行打乱
def shuffle_identical_groups(sequences, groups, shuffle_count):
    # 如果 shuffle_count 为 0，不打乱，直接返回原始顺序
    if shuffle_count == 0:
        return sequences
    shuffled_sequences = sequences[:]
    for group in groups.values():
        group_indices = [sequences.index(seq) for seq in group if seq in sequences]
        if len(group_indices) > 1:
            shuffled_positions = random.sample(group_indices, len(group_indices))
            for i, idx in enumerate(group_indices):
                shuffled_sequences[idx] = sequences[shuffled_positions[i]]
    return shuffled_sequences

# 计算连通性：基于相邻生境变化的次数
def calculate_connectivity(sequences, habitats):
    connectivity = 0
    n = len(sequences)

    for i in range(n):
        left = habitats[sequences[i]]
        right = habitats[sequences[(i + 1) % n]]

        # 只计算相邻的生境变化次数
        if left != right:
            connectivity += 1

    return connectivity

# 计算最小和最大连通性
def calculate_standard_connectivity(sequences, habitats):
    habitat_counts = Counter(habitats[seq] for seq in sequences)
    habitats_sorted = sorted(habitat_counts.keys())

    min_sequence = []
    max_sequence = []

    # 最小连通性：相同生境的聚在一起
    for habitat in habitats_sorted:
        min_sequence.extend([habitat] * habitat_counts[habitat])

    # 最大连通性：生境交替排列
    habitat_counts_copy = habitat_counts.copy()  # 深复制
    while sum(habitat_counts_copy.values()) > 0:
        for habitat in habitats_sorted:
            if habitat_counts_copy[habitat] > 0:
                max_sequence.append(habitat)
                habitat_counts_copy[habitat] -= 1

    # 创建虚拟的序列名与生境映射
    min_sequence_dict = {f"Seq{i}": min_sequence[i] for i in range(len(min_sequence))}
    max_sequence_dict = {f"Seq{i}": max_sequence[i] for i in range(len(max_sequence))}

    # 用虚拟的序列名计算连通性
    min_connectivity = calculate_connectivity(list(min_sequence_dict.keys()), min_sequence_dict)
    max_connectivity = calculate_connectivity(list(max_sequence_dict.keys()), max_sequence_dict)

    return min_connectivity, max_connectivity

# 计算归一化连通性，并返回最大、最小、平均值
def calculate_normalized_connectivity(sequences, habitats, groups, shuffle_count, max_sample_size):
    all_scores = []

    # 计算标准连通性
    min_connectivity, max_connectivity = calculate_standard_connectivity(sequences, habitats)

    if shuffle_count == 0:
        # 不打乱，直接计算原始数据的连通性
        connectivity = calculate_connectivity(sequences, habitats)
        all_scores.append(connectivity)
    else:
        for _ in range(shuffle_count):
            shuffled_sequences = shuffle_identical_groups(sequences, groups, shuffle_count)
            connectivity = calculate_connectivity(shuffled_sequences, habitats)
            all_scores.append(connectivity)

    min_score = np.min(all_scores)
    max_score = np.max(all_scores)
    avg_score = np.mean(all_scores)

    # 归一化
    if max_connectivity != min_connectivity:
        normalized_min = ((min_score - min_connectivity) / (max_connectivity - min_connectivity)) * (1 - 1/max_sample_size) + 1/max_sample_size
        normalized_max = ((max_score - min_connectivity) / (max_connectivity - min_connectivity)) * (1 - 1/max_sample_size) + 1/max_sample_size
        normalized_avg = ((avg_score - min_connectivity) / (max_connectivity - min_connectivity)) * (1 - 1/max_sample_size) + 1/max_sample_size
    else:
        normalized_min = normalized_max = normalized_avg = 1 / max_sample_size

    return normalized_avg

# 重采样序列和生境，并输出到Excel文件
def resample_sequences_habitats(df, gene_tree, sample_tree, iterations, output_file, max_sample_size=30):
    results = []

    groups = df['Group'].unique()
    for group in groups:
        group_df = df[df['Group'] == group]
        gene_sequences = group_df['Gene'].tolist()
        sample_sequences = group_df['Sample'].tolist()
        habitats = dict(zip(group_df['Gene'], group_df['Habitat']))
        sample_habitats = dict(zip(group_df['Sample'], group_df['Habitat']))

        # 获取每个Habitat的样本数
        habitat_counts = group_df.groupby('Habitat').size()

        # 如果max_sample_size为0，使用每个组中最小的Habitat数
        if max_sample_size == 0:
            min_habitat_count = habitat_counts.min()
        else:
            min_habitat_count = min(habitat_counts.min(), max_sample_size)

        # 确保随机化，每次循环重新设置随机种子
        for i in range(iterations):
            random.seed()  # 不传递参数以重新生成随机种子

            # 对Gene进行按Habitat抽样，使用最小Habitat的样本数进行抽平
            gene_groups = group_sequences_by_distance(gene_tree, gene_sequences)
            balanced_genes = sample_from_groups_by_habitat(gene_groups, min_habitat_count, habitats, gene_tree)

            # 对Sample进行按Habitat抽样，使用最小Habitat的样本数进行抽平
            sample_groups = group_sequences_by_distance(sample_tree, sample_sequences)
            balanced_samples = sample_from_groups_by_habitat(sample_groups, min_habitat_count, sample_habitats, sample_tree)

            # 计算Gene的连通性并归一化
            gene_avg = calculate_normalized_connectivity(balanced_genes, habitats, gene_groups, 0, min_habitat_count)

            # 计算Sample的连通性并归一化
            sample_avg = calculate_normalized_connectivity(balanced_samples, sample_habitats, sample_groups, 0, min_habitat_count)

            results.append({
                'Iteration': i + 1,
                'Group': group,
                'Gene Avg': gene_avg,
                'Sampled Genes': balanced_genes,
                'Sampled Samples': balanced_samples
            })

    # Save the results to an Excel file
    df_result = pd.DataFrame(results)
    df_result.to_excel(output_file, index=False)

# 主函数，运行整个分析流程
def main(habitat_file, gene_tree_file, sample_tree_file, iterations=10, output_file='Result_pig.xlsx', max_sample_size=30):
    df = pd.read_csv(habitat_file)
    gene_tree = parse_tree(gene_tree_file)
    sample_tree = parse_tree(sample_tree_file)

    # Run the resampling and connectivity analysis
    resample_sequences_habitats(df, gene_tree, sample_tree, iterations, output_file, max_sample_size)

if __name__ == "__main__":
    habitat_file = "Habitat_pig.csv"
    gene_tree_file = "Gene_pig.tree"
    sample_tree_file = "Sample_pig.tree"
    max_sample_size = 30  # 如果为0，则按最小生境样本数抽样
    main(habitat_file, gene_tree_file, sample_tree_file, max_sample_size=max_sample_size)
